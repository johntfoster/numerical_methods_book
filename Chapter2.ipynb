{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Solving Systems of Linear Equations\n",
      "\n",
      "\n",
      "Recall the three basic rules for matrix manipulation from linear algebra:\n",
      "\n",
      "    1. Switching two rows or columns does not change the solution of the linear system.\n",
      "    2. Any row can be multiplied by a constant without changing the solution of the linear system.\n",
      "    3. Any row or linear multiple of a row can be added/subtracted to/from another row without changing the solution of the linear system.\n",
      "        \n",
      "Consider the system of equations:\n",
      "\n",
      "3 $x_1$ + 8 $x_2$ = 10\n",
      "   \n",
      "4 $x_1$ + 6 $x_2$ = 2\n",
      "\n",
      "We can write in matrix form:\n",
      "\n",
      "$$\n",
      "\\begin{pmatrix}\n",
      "3 & 8 & 10\\\\ 4 & 6 & 2\n",
      "\\end{pmatrix}\n",
      "$$\n",
      "\n",
      "We can multiply the 1st row by -4/3 and add it to the 2nd equation, etc...\n",
      "\n",
      "$$\n",
      "-\\frac{4}{3}\n",
      "\\begin{pmatrix}\n",
      "3 & 8 & 10 \\\\ 4 & 6 & 2\n",
      "\\end{pmatrix} \\rightarrow  \n",
      "\\begin{pmatrix}\n",
      "-4 & -\\frac{32}{3} & -\\frac{40}{3} \\\\ 4 & 6 & 2\n",
      "\\end{pmatrix} \\rightarrow \n",
      "\\begin{pmatrix}\n",
      "-4 & -\\frac{32}{3} & -\\frac{40}{3} \\\\ 0 & -\\frac{14}{3} & -\\frac{34}{3}\n",
      "\\end{pmatrix} \\rightarrow\n",
      "\\begin{matrix} - \\frac{1}{4} \\\\ -\\frac{3}{14} \\end{matrix}\n",
      "\\begin{pmatrix}\n",
      "-4 & -\\frac{32}{3} & -\\frac{40}{3} \\\\ 0 & -\\frac{14}{3} & -\\frac{34}{3}\n",
      "\\end{pmatrix} \\rightarrow\n",
      "\\begin{pmatrix}\n",
      "1 & \\frac{8}{3} & \\frac{10}{3} \\\\ 0 & 1 & \\frac{17}{7}\n",
      "\\end{pmatrix}\n",
      "$$\n",
      "\n",
      "By inspection we can see that $x_2 = \\frac{17}{2}$ we can then substitue this solution into a modified first equation\n",
      "\n",
      "$$\n",
      "x_1 + \\frac{8}{3}x_2 = \\frac{10}{3}\n",
      "$$\n",
      "\n",
      "and solve for $x_1$. The final solution is $x_1 = -\\frac{22}{7}$,  $x_2 = \\frac{17}{7}$. \n",
      "\n",
      "This procedure is called Gaussian elimination.\n",
      "\n",
      "## Stability of Gaussian Elimination\n",
      "\n",
      "Recall that another way of writing a linear system in matrix form in $A\\overline{x} = \\overline{b}$ where $A$ is the coefficient matrix, $\\overline{x}$ is the vector of unknowns, and $\\overline{b}$ is the right-hand side vector.\n",
      "\n",
      "One method of determining if a linear system has a solution is to take the determinate of the $A$ matrix. If the determinate of $A = 0$ then the matrix is said to be *singular* and the system either has no solution or an infinite number of solutions.\n",
      "\n",
      "A matrix that is near-singular is said to be *ill-conditioned* whereas one that is far from singular is said to be *well-conditioned*. There is a notion of *condition number* that mathematically quantifies a matrix's amenability to numeric computations with matrices having high condition numbers being well condidioned and matrices with low condition numbers being ill conditioned. We will return to condition numbers in more detail when we discuss singlar value decomposition. Below is an example of two matrices $A$ and $B$. $A$ is an ill-conditioned matrix, $B$ is well-conditioned."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "If we change the last entry of $A$ to $a_{22} = 1$, it is singular."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "To illustrate the ill-conditioning of $A$ in another way let's consider two very close right-hand side vectors, $\\vec{b}$:\n",
      "\n",
      "$$\n",
      "\\begin{matrix}\n",
      "x_1 +& x_2& = 2 &and& x_1 +& x_2 &=& 2\\\\\n",
      "x_1 +&1.0001x_2& = 2 &and& x_1 +& 1.0001x_2 &=& 2.0001\n",
      "\\end{matrix}\n",
      "$$\n",
      "\n",
      "The system of equations on the left has the solution $x_1 = 2, x_2 = 0$, and the system of equations on the right has the solution $x_1 = x_2 = 1$. Notice that a change in the fifth digit of $\\vec{b}$ was amplified to a change in the first digit of the solution. Even the most robust numerical method will have trouble with this sensitivity to small perturbations.\n",
      "\n",
      "Now I would like to illustrate that even a well-conditioned matrix like $B$ can be ruined by a poor algorithm. Consider the matrix $B$ with the right-hand side vector, $\\vec{b} = [1,2]$ and let's proceed through Gaussian elimination. We will start by multiplying $-10000$ times the first row and adding it to the second.\n",
      "\n",
      "$$\n",
      "\\begin{pmatrix} 0.0001 & 1 & 1\\\\1&1&2\\end{pmatrix} \\rightarrow\n",
      "\\begin{pmatrix} 0.0001&1&1\\\\0&-9999&-9998\\end{pmatrix} \\rightarrow\n",
      "_{-\\frac{1}{9999}}\n",
      "\\begin{pmatrix} 0.0001&1&1\\\\0&-9999&-9998 \\end{pmatrix} \\rightarrow\n",
      "\\begin{pmatrix} 0.0001&1&1\\\\0&1&0.9999\\end{pmatrix}\n",
      "$$\n",
      "\n",
      "by inspection we see that $x_2 = 0.9999$, now we can back substitute into the first equation\n",
      "\n",
      "$x_1+10000x_2 = 10000$\n",
      "\n",
      "and solve for $x_1$. The final solution is $x_1 = 1, x_2 = 0.9999$. Now let us do the same computation, this time on a computer that rounds the results of the computations to 3 decimal places (a machine epsilon of 0.0001), the resulting matrix after manipulations would be:\n",
      "\n",
      "$$\n",
      "\\begin{pmatrix} 0.0001&1&1\\\\0&1&1\\end{pmatrix}\n",
      "$$\n",
      "\n",
      "by inspection we see that $x_2 = 1$ and backsubstitution yields $x_1 = 0$. This is the incorrect solution brought on by the small pivot 0.0001. For this matrix Gaussian elimination is a poor algorithm. Fortunately there is an easy solution: exhange rows.\n",
      "\n",
      "Now lets do the same computation on teh same computer that rounds off the computation to 3 decimal places. This time we will exchange the rows before proceeding with the Gaussian elimination. Eliminating some detail we can see:\n",
      "\n",
      "$$\n",
      "\\begin{pmatrix}1&1&2\\\\0.0001&1&1\\end{pmatrix}\\rightarrow\n",
      "\\begin{pmatrix}1&1&2\\\\0&0.999&0.999\\end{pmatrix}\\rightarrow\n",
      "\\begin{pmatrix}1&1&2\\\\0&1&1\\end{pmatrix}\n",
      "$$\n",
      "\n",
      "by inspection we see that $x_2 = 1$, and back substitution yields $x_1 = 1$. We can see that this solution differs from the exact solution only by the roundoff error of the computer. Exchanging rows to obtain the largest possible pivot is called *partial pivoting*. Exchanging both rows and columns to obtain the largest possible pivot is called *full pivoting*. Full pivoting will result in the most stable algorithm but requires more computations and the requirement of tracking column permutations. For most matrices partial pivoting is sufficient enough for stability.\n",
      "\n",
      "## Gauss-Jordan Elimination\n",
      "\n",
      "Gauss-Jordan elimination simply adds steps to the simple Gauss elimination procedure to produce a matrix that is in *reduced row echelon form*. This is done by eliminating values both above and below the pivots and ensuring that each pivot has the value 1. Starting where we ended on the exact solution of matrix $B$ earlier we can simply add two steps to produce a reduced row echelon matrix. First we subtract the second row from the first:\n",
      "\n",
      "$$\n",
      "\\begin{pmatrix}0.0001&1&1\\\\0&1&0.9999\\end{pmatrix}\\rightarrow\\begin{pmatrix}0.0001&0&0.0001\\\\0&1&0.9999\\end{pmatrix}\n",
      "$$\n",
      "\n",
      "then we normalize the first equation to produce a 1 on the pivot:\n",
      "\n",
      "$$\n",
      "^{\\frac{1}{0.0001}}\n",
      "\\begin{pmatrix}0.0001&0&0.0001\\\\0&1&0.9999\\end{pmatrix}\\rightarrow\n",
      "\\begin{pmatrix}1&0&1\\\\0&1&0.9999\\end{pmatrix}\n",
      "$$\n",
      "\n",
      "now by inspection (and without the use of back substitution) we can see that the solutions are $x_1 = 1$ and $x_2 = 0.9999$ just as before.\n",
      "\n",
      "For a linear system with $n$ equations and $m$ unknowns. The innermost loops (one subtraction and one multiplication) of a Gauss-Jordan elimination routine are executed $n^3$ and $n^2$ $m$ times. The corresponding loops in a Gaussian scheme are executed $\\frac{1}{3}n^3$ and $\\frac{1}{2}n^2m$ times, followed by a back substitution for a similar loop (one subtraction and one multiplication) taht occurs $\\frac{1}{2}n^2$ times. If there are far more equations that unknowns the Gaussian scheme with back substitution enjoys roughly a factor of 3 advantage over the Gauss-Jordan, for $n=m$ Gaussian elimination with back substitution enjoys a factor of 6 advantage over Gauss-Jordan. For matrix inversion (discussed later) the two methods have identical efficiencies.\n",
      "\n",
      "## Pseudocode for Gauss Elimination with Back Substitution and Partial Pivoting\n",
      "\n",
      "Consider a linear system with $n$ equations and $n$ unknowns:\n",
      "$$\n",
      "E_1: a_{11}x_1+a_{12}x_2+...+a_{1n}x_n = a_{1,n+1}\\\\\n",
      "E_2: a_{21}x_1+a_{22}x_2+...+a_{2n}x_n = a_{2,n_1}\\\\\n",
      "...\\\\\n",
      "E_n: a_{n1}x_1+a_{n2}x_2+...+a_{nn}x_n = a_{n,n+1}\n",
      "$$\n",
      "\n",
      "|Steps|   |   |\n",
      "|-----|---|---|\n",
      "|1    |For $i = 1$, ..., $n$ do Steps 2-4|loop over columns in progression|\n",
      "|2    |Find $p$, where $p$ is the largest number with $i\\leq p\\leq n$ | search the current pivot and rows below the current one for the largest value|\n",
      "|3    |If $p\\neq i$, then exchange row $i$ with row $p$ | now the largest value in the column is the new pivot|\n",
      "|4    |For $j=i+1, ..., n$ do Steps 5-6|the innermost loop, multiplication and subtraction|\n",
      "|5    |set $m_{ji} = a_{ji}/a_{ii}$| |\n",
      "|6    |Perform $E_j = ( E_j - m_{ji}E_i)$| |\n",
      "|7    |Set $x_n = a_{n,n+1}/a_{nn}$|start back substitution|\n",
      "|8    |For $i = n-1, ..., 1$ set $x_i=\\frac{a_{i,n+1}-\\sum\\nolimits_{j=i+1}a_{ij}x_j}{a_{ii}}$| |\n",
      "\n",
      "## Pseudocode for Gauss-Jordan Elimination with Partial Pivoting\n",
      "\n",
      "Consdier a linear system with $n$ equations and $n$ unknowns:\n",
      "\n",
      "$$\n",
      "E_1: a_{11}x_1+a_{12}x_2+...+a_{1n}x_n = a_{1,n+1}\\\\\n",
      "E_2: a_{21}x_1+a_{22}x_2+...+a_{2n}x_n = a_{2,n_1}\\\\\n",
      "...\\\\\n",
      "E_n: a_{n1}x_1+a_{n2}x_2+...+a_{nn}x_n = a_{n,n+1}\n",
      "$$\n",
      "\n",
      "|Steps| | |\n",
      "|-----|-|-|\n",
      "|1|For $i=1,...,n$ do Steps 2-4|loop over columns in progression|\n",
      "|2|Find $p$, where $p$ is the largest number with $i\\leq p \\leq n$|search the current pivot and the rows below the current one for the largest value|\n",
      "|3|If $p\\neq i$, then exchange row $i$ with row $p$|now the largest value in the column is the new pivot|\n",
      "|4|For $j=1,...,n$ do Steps 5-6|the innermost loop, multiplication and subtraction|\n",
      "|5|Perform $E_i = \\frac{1}{a_{ii}E_i}$| |\n",
      "|6|If $i \\neq j$ perform $E_j-a_{ji}E_i$| |\n",
      "|7|For $i = 1,...,n$ set $x_i = a_{i,n+1}$| |\n",
      "\n",
      "## Matrix Inversion with Gauss-Jordan scheme\n",
      "\n",
      "For matrix inversion, both the Gauss elimination with back-substitution and Gauss-Jordan schemes described previously have identical efficiencies. For this reason we will, for simplicities sake, only consider the process for matrix inversion using the Gauss-Jordan scheme.\n",
      "\n",
      "All we have to do is augment an $A$ matrix with an identity matrix of the same dimensions and proceed with Gauss-Jordan elimination exactly as we have done previously. Consider the following $A$ matrix:\n",
      "\n",
      "$A = \\begin{pmatrix}1&3&2\\\\2&4&5\\\\10&7&3\\end{pmatrix}$ we augment it with a 3x3 identity matrix $A|I$ and proceed with Gauss-Jordan Elimination.\n",
      "\n",
      "$$\n",
      "\\begin{pmatrix}\n",
      "1&3&2&1&0&0\\\\2&4&5&0&1&0\\\\10&7&3&0&0&1\n",
      "\\end{pmatrix} \\rightarrow\n",
      "\\begin{pmatrix}\n",
      "1&3&2&1&0&0\\\\0&-2&1&-2&1&0\\\\0&-23&-17&-10&0&1\n",
      "\\end{pmatrix}\\rightarrow\n",
      "$$\n",
      "\n",
      "$$\n",
      "\\begin{pmatrix}\n",
      "1&0&\\frac{7}{2}&-2&\\frac{3}{2}&0\\\\0&1&-\\frac{1}{2}&1&-\\frac{1}{2}&0\\\\0&0&-\\frac{57}{2}&13&-\\frac{23}{2}&1\n",
      "\\end{pmatrix}\\rightarrow\n",
      "\\begin{pmatrix}\n",
      "1&0&\\frac{7}{2}&-2&\\frac{3}{2}&0\\\\0&1&-\\frac{1}{2}&1&-\\frac{1}{2}&0\\\\0&0&-\\frac{57}{2}&13&-\\frac{23}{2}&1\n",
      "\\end{pmatrix} \\rightarrow\n",
      "\\begin{pmatrix}\n",
      "1&0&0&-\\frac{23}{57}&\\frac{5}{57}&\\frac{7}{57}\\\\0&1&0&\\frac{44}{57}&-\\frac{17}{57}&-\\frac{1}{57}\\\\0&0&1&-\\frac{26}{57}&-\\frac{23}{57}&-\\frac{2}{57}\n",
      "\\end{pmatrix}\n",
      "$$\n",
      "\n",
      "Here I am neglecting the partial-pivoting which you would want to include to ensure stability, but we can see by inspection the right side 3x3 submatrix is the inverse of $A$. Verifying using the inverse routine build into ####*Python*#### we have:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "A = [ [1,3,2],[2,4,5],[10,7,3] ] #creates matrix\n",
      "b = A**-1"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "TypeError",
       "evalue": "unsupported operand type(s) for ** or pow(): 'list' and 'int'",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-23-7db21b4a2c2d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mA\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m]\u001b[0m \u001b[0;31m#creates matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mA\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
        "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for ** or pow(): 'list' and 'int'"
       ]
      }
     ],
     "prompt_number": 23
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from numpy import matrix\n",
      "from numpy import linalg\n",
      "A = matrix( [[1,3,2],[11,12,13],[21,22,23]] ) #creates matrix\n",
      "print A.I                                     #Inverse of A"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[[-0.33333333 -0.83333333  0.5       ]\n",
        " [ 0.66666667 -0.63333333  0.3       ]\n",
        " [-0.33333333  1.36666667 -0.7       ]]\n"
       ]
      }
     ],
     "prompt_number": 20
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": []
    }
   ],
   "metadata": {}
  }
 ]
}