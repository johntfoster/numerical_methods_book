{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "remove_cell"
    ]
   },
   "source": [
    "# Iterative Methods for Solving Linear Systems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Iterative techniques are rarely used for solving linear systems of small dimension because the computation time required for convergence usually exceeds that required for direct methods such as Gaussian elimination.  However, for very large systems, especially sparse systems (systems with a high percentage of 0 entries in the matrix), these iterative techniques can be very efficient in terms of computational run times and memory usage.\n",
    "\n",
    "An iterative technique starts to solve the matrix equation $\\mathbf{A}\\vec{x} = \\vec{b}$ starts with an initial approximation $\\vec{x^0}$ and generates a sequence of vectors $\\{\\vec{x}^1,\\vec{x}^2,\\ldots, \\vec{x}^N\\}$ that converges to $\\vec{x}$ as $N\\rightarrow\\infty$. These techniques involve a process that converts the system $\\mathbf{A}\\vec{x}=\\vec{b}$ to an equivalent system of the form $\\vec{x}=\\mathbf{T}\\vec{x}+\\vec{c}$. The process then follows, for an initial guess $\\vec{x}^0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": [
     "remove_cell"
    ]
   },
   "source": [
    "Starting with an initial guess $\\vec{x}_0$, we convert\n",
    "\n",
    "$$\n",
    "\\mathbf{A}\\vec{x} = \\vec{b} \\qquad \\to \\qquad \\vec{x} = \\mathbf{T}\\vec{x} + \\vec{c}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\\begin{align}\n",
    "\\vec{x}^1 &= \\mathbf{T}\\vec{x}^0 + \\vec{c}\\\\\n",
    "\\vec{x}^2 &= \\mathbf{T}\\vec{x}^1 + \\vec{c}\\\\\n",
    "&\\phantom{=}\\vdots\\\\\n",
    "\\vec{x}^N &= \\mathbf{T}\\vec{x}^{N-1} + \\vec{c}\\\\\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "We will stop the iteration when some convergence criterion has been reached. A popular convergence criterion uses the $L_{\\infty}$norm. The $L_{\\infty}$ norm is a metric that represents the greatest length or size of a vector or matrix component. Expressed mathematically,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "remove_cell"
    ]
   },
   "source": [
    "**Example convergence criterion**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "\\vec{x} = [x_1, x_2, ..., x_n]^T\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "\\Vert \\vec{x}\\Vert_{L_\\infty} = \\max \\vert x_i\\vert \\quad \\forall \\quad i = 1, 2, ..., n\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## An example using an iterative method\n",
    "\n",
    "Consider the system:\n",
    "\n",
    "$$\n",
    "\\begin{matrix} E_1: & 10x_1 & -x_2 & +2x_3 & & =& 6\\\\E_2: & -x_1 & +11x_2 & -x_3 & +3x_4 & =&25\\\\E_3: & 2x_1 & -x_2 & + 10x_3 & -x_4 & =&-11\\\\E_4: &  & -3x_2 & -x_3 & +8x_4 & = &15\\end{matrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Let us solve each equation, $E_j$, for the variable $x_j$.\n",
    "\n",
    "$$\n",
    "\\begin{matrix} E_1: & x_1 =&  &\\frac{1}{10}x_2& - \\frac{1}{5}x_3 &  & +\\frac{3}{5}\\\\\n",
    "E_2: & x_2 = &\\frac{1}{11}x_1 & &\\frac{1}{11}x_3 & -\\frac{3}{11}x_4 & +\\frac{25}{11}\\\\\n",
    "E_3: & x_3 = &-\\frac{1}{5}x_1 & +\\frac{1}{10}x_2 & & +\\frac{1}{10}x_4 & -\\frac{11}{10}\\\\\n",
    "E_4: & x_4 = & & -\\frac{3}{8}x_2 & + \\frac{1}{8}x_3 & & +\\frac{15}{8} \\end{matrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "using the notation introduced previously we have"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$\n",
    "\\vec{x}^1 = \\begin{bmatrix} 0 & \\frac{1}{10} & -\\frac{1}{5} & 0\\\\\n",
    "\\frac{1}{11} & 0 & \\frac{1}{11} & -\\frac{3}{11}\\\\\n",
    "-\\frac{1}{5} & \\frac{1}{10} & 0 & \\frac{1}{10}\\\\\n",
    "0 & -\\frac{3}{8} & \\frac{1}{8} & 0 \\end{bmatrix}\\vec{x}_0 + \\begin{bmatrix}\\frac{3}{5}\\\\\\frac{25}{11}\\\\-\\frac{11}{10}\\\\\\frac{15}{8}\\end{bmatrix},$$ \n",
    "\n",
    "for\n",
    "\n",
    "$$\\vec{x}^0 = \\begin{bmatrix}0\\\\0\\\\0\\\\0\\end{bmatrix}, \\qquad \\vec{x}^1 = \\begin{bmatrix}0.6000\\\\2.2727\\\\-1.1000\\\\1.8750\\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "we repeat this process until the desired convergence has been reached. This technique is called the Jacobi iterative method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Psuedocode for Jacobi iteration\n",
    "\n",
    "For the matrix equation $\\mathbf{A} \\vec{x} = \\vec{b}$ with an initial guess $\\vec{x}^0$.\n",
    "\n",
    "\\begin{equation}\n",
    "A = \\begin{bmatrix}a_{11}&a_{12}&...&a_{1n}\\\\a_{21}&\\ddots& &\\vdots\\\\\\vdots& &\\ddots&\\vdots\\\\a_{n1}&...&...&a_{nn}\\end{bmatrix}, \\quad \\vec{b} = \\begin{bmatrix} b_{1} \\\\ b_{2} \\\\ \\vdots \\\\ b_{n}\\end{bmatrix}, \\quad \\vec{x}^{0} = \\begin{bmatrix} x^0_{1} \\\\ x^0_{2} \\\\ \\vdots \\\\ x^0_{n}\\end{bmatrix}, \n",
    "\\end{equation}\n",
    "\n",
    "|Steps| |\n",
    "| --: | |\n",
    "| 1.  | While $\\frac{\\Vert \\vec{x}_k - \\vec{x}_{k-1} \\Vert_{L_\\infty}}{\\Vert \\vec{x} \\Vert_{L_\\infty}}> tolerance$ do Step 2\n",
    "| 2.  | $\\phantom{--}$ For $i = 1, 2, \\ldots, n$ do Step 3\n",
    "| 3.  | $$\\phantom{----} x^k_i = \\left.\\left( -\\sum_{\\substack{j=1 \\\\ i \\ne j}}^n a_{ij} x^k_j  + b_i\\right) \\middle/ a_{ii} \\right.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### Python/NumPy implementation of Jacobi iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "remove_cell"
    ]
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def jacobi(A, b, tolerance=1e-10, max_iterations=10000):\n",
    "    \n",
    "    x = np.zeros_like(b, dtype=np.double)\n",
    "    \n",
    "    T = A - np.diag(np.diagonal(A))\n",
    "    \n",
    "    for k in range(max_iterations):\n",
    "        \n",
    "        x_old  = x.copy()\n",
    "        \n",
    "        x[:] = (b - np.dot(T, x)) / np.diagonal(A)\n",
    "        \n",
    "        if np.linalg.norm(x - x_old, ord=np.inf) / np.linalg.norm(x, ord=np.inf) < tolerance:\n",
    "            break\n",
    "            \n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Observations on the Jacobi iterative method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Let's consider a matrix $\\mathbf{A}$, in which we split into three matrices, $\\mathbf{D}$, $\\mathbf{U}$, $\\mathbf{L}$, where these matrices are diagonal, upper triangular, and lower triangular respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "A = \\begin{bmatrix}a_{11}&a_{12}&\\ldots&a_{1n}\\\\a_{21}&\\ddots& &\\vdots\\\\\\vdots& &\\ddots&\\vdots\\\\a_{n1}&\\ldots&\\ldots&a_{nn}\\end{bmatrix}, D = \\begin{bmatrix}a_{11}&0&\\ldots&0\\\\0&\\ddots& &\\vdots\\\\\\vdots& &\\ddots&\\vdots\\\\0&\\ldots&\\ldots&a_{nn}\\end{bmatrix}, \\\\\n",
    "L = \\begin{bmatrix}0&0&\\ldots&0\\\\-a_{21}&\\ddots& &\\vdots\\\\\\vdots& &\\ddots&\\vdots\\\\-a_{n1}&\\ldots&-a_{n(n-1)}&0\\end{bmatrix}, U = \\begin{bmatrix}0&-a_{12}&\\ldots&-a_{1n}\\\\\\vdots&\\ddots& &\\vdots\\\\\\vdots& &\\ddots&-a_{(n-1)n}\\\\0&\\ldots&\\ldots&0\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "If we let $\\mathbf{A = D-L-U}$, then the matrix equation $\\mathbf{A}\\vec{x} = \\vec{b}$ becomes\n",
    "\n",
    "\\begin{equation}\n",
    "\\left(\\mathbf{D-L-U}\\right)\\vec{x} = \\vec{b} \\quad \\mathrm{or} \\quad \\mathbf{D}\\vec{x} = \\left(\\mathbf{L+U}\\right)\\vec{x}+\\vec{b}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "if $\\mathbf{D}^{-1}$ exists, that implies $a_{jj} \\neq 0$, then"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$\\vec{x} = \\mathbf{D}^{-1}\\left(\\mathbf{L+U}\\right)\\vec{x}+\\mathbf{D}^{-1}\\vec{b}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "The results in the matrix form of the Jacobi iteration method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\\begin{equation}\n",
    "\\vec{x}^k = \\mathbf{D}^{-1}\\left(\\mathbf{L+U}\\right)\\vec{x}^{k-1}+\\mathbf{D}^{-1}\\vec{b}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "We can see that one requirement for the Jacobi iteration to work is for $a_{ii} \\neq 0$. This may involve row exchanges before iterating for some linear systems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Gauss-Seidel iteration: An improvement to the Jacobi iterative method "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "During the Jacobi iteration we always use the components of $\\vec{x}^{k-1}$ to compute $\\vec{x}^{k}$ but for $i > 1, {x}^{k}_1, \\ldots, {x}^{k}_{i-1}$ are already computed and are most likely the best approximations of the real solution.  Therefore, we can calculate ${x}^{k}_i$ using the most recently calculated values when available. This technique is called *Gauss-Seidel iteration*. The pseudocode is as follows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Pseudocode for Gauss-Seidel iteration\n",
    "\n",
    "For the matrix equation $\\mathbf{A} \\vec{x} = \\vec{b}$ with an initial guess $\\vec{x}^0$.\n",
    "\n",
    "$$\n",
    "A=\\begin{bmatrix}a_{11}&a_{12}&\\ldots&a_{1n}\\\\a_{21}&\\ddots& &\\vdots\\\\\\vdots& &\\ddots&\\vdots\\\\a_{n1}&\\ldots&\\ldots&a_{nn}\\end{bmatrix}, \\bar{b} = \\begin{bmatrix}b_1\\\\b_1\\\\\\vdots\\\\b_n\\end{bmatrix}, \\bar{x}_0 = \\begin{bmatrix}x^{0}_1\\\\x^{0}_2\\\\\\vdots\\\\x^{0}_n\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "\n",
    "| Steps | |\n",
    "| --: | :-- |\n",
    "| 1.  | While $\\frac{\\Vert\\vec{x}^{k}-\\vec{x}^{k-1}\\Vert_{L_\\infty}}{\\Vert\\bar{x}^{k}\\Vert_{L_\\infty}} > tolerance$ do Step 2\n",
    "| 2.  | $\\phantom{--}$ For $i = 1, 2, ..., n$ do Step 3\n",
    "| 3.  | $$\\phantom{----} x^{k}_i = \\left.\\left(-\\sum_{j=1}^{i-1}a_{ij}x^{k}_j-\\sum_{j=i+1}^{n}a_{ij}x^{k-1}_j+b_i\\right) \\middle/ {a_{ii}}\\right.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### Python/NumPy implementation of Gauss-Seidel iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def gauss_seidel(A, b, tolerance=1e-10, max_iterations=10000):\n",
    "    \n",
    "    x = np.zeros_like(b, dtype=np.double)\n",
    "    \n",
    "    #Iterate\n",
    "    for k in range(max_iterations):\n",
    "        \n",
    "        x_old  = x.copy()\n",
    "        \n",
    "        #Loop over rows\n",
    "        for i in range(A.shape[0]):\n",
    "            x[i] = (b[i] - np.dot(A[i,:i], x[:i]) - np.dot(A[i,(i+1):], x_old[(i+1):])) / A[i ,i]\n",
    "            \n",
    "        #Stop condition \n",
    "        if np.linalg.norm(x - x_old, ord=np.inf) / np.linalg.norm(x, ord=np.inf) < tolerance:\n",
    "            break\n",
    "            \n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Observations on the Gauss-Seidel iterative method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Let's consider a matrix $\\mathbf{A}$, in which we split into three matrices, $\\mathbf{D, U, L}$, where these matrices are diagonal, upper triangular, and lower triangular respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "A = \\begin{bmatrix}a_{11}&a_{12}&\\ldots&a_{1n}\\\\a_{21}&\\ddots& &\\vdots\\\\\\vdots& &\\ddots&\\vdots\\\\a_{n1}&\\ldots&\\ldots&a_{nn}\\end{bmatrix}, D = \\begin{bmatrix}a_{11}&0&\\ldots&0\\\\0&\\ddots& &\\vdots\\\\\\vdots& &\\ddots&\\vdots\\\\0&\\ldots&\\ldots&a_{nn}\\end{bmatrix}, \\\\ L = \\begin{bmatrix}0&0&\\ldots&0\\\\-a_{21}&\\ddots& &\\vdots\\\\\\vdots& &\\ddots&\\vdots\\\\-a_{n1}&\\ldots&-a_{n(n-1)}&0\\end{bmatrix}, U = \\begin{bmatrix}0&-a_{12}&\\ldots&-a_{1n}\\\\\\vdots&\\ddots& &\\vdots\\\\\\vdots& &\\ddots&-a_{(n-1)n}\\\\0&\\ldots&\\ldots&0\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "We will leave, as an exercise for the student, the derivation, but the matrix equation for the Gauss-Seidel iteration method is as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$\\vec{x}^k = \\left(\\mathbf{D-L}\\right)^{-1}\\mathbf{U}\\vec{x}^{k-1}+\\left(\\mathbf{D-L}\\right)^{-1}\\vec{b}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "In order for the lower triangular matrix $\\mathbf{D-L}$ to be invertible it is necessary and sufficient for $a_{ii}\\neq 0$. As before, this may involve row exchanges before iterating for some linear systems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Convergence of iterative methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "First, a definition, the *spectral radius*, $\\rho$, of matrix $\\mathbf{A}$ is the maximum of the absolute values of the matrix $\\mathbf{A}$'s eigenvalues."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "remove_cell"
    ]
   },
   "source": [
    "The *spectral radius* $\\rho$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\rho\\left(\\mathbf{A}\\right) = \\max\\vert\\lambda_i\\vert$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "where the $\\lambda_i$'s are the eigenvalues of $\\mathbf{A}$.\n",
    "\n",
    "Now, if $\\rho\\left(\\mathbf{A}\\right) < 1$, then $(\\mathbf{I-A})^{-1}$ exists, and "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$(\\mathbf{I-A})^{-1} = \\mathbf{I + A + A}^2 + \\ldots = \\sum_{j=0}^{\\infty}\\mathbf{A}^j$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "We can prove this, starting with the eigenvalue equation\n",
    "\n",
    "$$\\mathbf{A}\\vec{x} = \\lambda \\vec{x} \\rightarrow (\\mathbf{I-A})\\vec{x} = (1-\\lambda)\\vec{x}$$\n",
    "\n",
    "$\\lambda$ is an eigenvalue of $\\mathbf{A}$, exactly when $(1-\\lambda)$ is an eigenvalue of $\\mathbf{I-A}$. But $\\vert\\lambda\\vert \\leq \\rho\\left(\\mathbf{A}\\right) < 1$, therefore $1$ cannot be an eigenvalue of $\\mathbf{A}$, and $0$ cannot be an eigenvalue of $\\mathbf{I-A}$. A matrix in which none of the eigenvalues are zero is always invertible, therefore $\\left(\\mathbf{I-A}\\right)^{-1}$ exists. There also exists an identity which states that a matrix $\\mathbf{A}$ is *convergent* if $\\rho\\left(\\mathbf{A}\\right) < 1$, which implies that $\\lim_{n\\to\\infty}\\mathbf{A}^n\\vec{x} = 0$ for all $\\vec{x}$. Now, let\n",
    "\n",
    "$$\\mathbf{S}_m = \\mathbf{I} + \\mathbf{A} + \\mathbf{A}^2 + \\ldots + \\mathbf{A}^m$$. \n",
    "\n",
    "Then,\n",
    "\n",
    "$$\\left(\\mathbf{I-A}\\right)\\mathbf{S}_m = \\left(1+\\mathbf{A+A}^2+\\ldots + \\mathbf{A}^m)-(\\mathbf{A+A}^2+\\ldots+\\mathbf{A}^{m+1}\\right)=\\left(\\mathbf{I-A}^{m+1}\\right).$$\n",
    "\n",
    "Since $\\mathbf{A}$ is convergent, we can see that by taking the limit of both sides\n",
    "\n",
    "$$\\lim_{m\\to\\infty}\\left(\\mathbf{I-A}\\right)\\mathbf{S}_m=\\lim_{m\\to\\infty}\\left(\\mathbf{I-A}^{m+1}\\right)=\\mathbf{I},$$\n",
    "\n",
    "therefore\n",
    "\n",
    "$$\\left(\\mathbf{I-A}\\right)^{-1} = \\lim_{m\\to\\infty}\\mathbf{S}_m = \\sum_{j=0}^{\\infty}\\mathbf{A}^j$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Now for any $\\bar{x}_0\\in\\mathbb{R}^n$ the sequence $\\{\\bar{x}^k\\}_{k=0}^{\\infty}$ computed by\n",
    "\n",
    "$$\\vec{x}^k = \\mathbf{T}\\vec{x}^{k-1}+\\vec{c},$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "for each $k\\geq 1$\n",
    "\n",
    "Converges to the unique solution of $\\vec{x} = \\mathbf{T} \\vec{x} + \\vec{c}$ if and only if $\\rho \\left(\\mathbf{T}\\right) < 1$.  We can prove this, first we will assume that $\\rho\\left(T\\right) < 1$. Then,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "\\vec{x}^k &= \\mathbf{T}\\vec{x}^{k-1}+\\vec{c} \\\\\n",
    "&= \\mathbf{T}\\left(\\mathbf{T}\\vec{x}^{k-2} + \\vec{c}\\right) + \\vec{c} \\\\\n",
    "&= \\mathbf{T}^2 \\vec{x}^{k-2}+\\left(\\mathbf{T-I}\\right)\\vec{c} \\\\\n",
    "&\\vdots \\\\\n",
    "&= \\mathbf{T}^k \\vec{x}^0 + (\\mathbf{T}^{k-1} + \\ldots + \\mathbf{T + I})\\bar{c}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Since $\\rho\\left(\\mathbf{T}\\right) < 1$, $\\mathbf{T}$ is convergent, then from what we learned previously, it follows\n",
    "\n",
    "$$\\lim_{k\\to\\infty}\\vec{x}^k = \\lim_{k\\to\\infty}\\mathbf{T}^k\\vec{x}^0 + \\left(\\sum_{j=0}^{\\infty}\\mathbf{T}^j\\right)\\vec{c} = \\mathbf{0} + \\left(\\mathbf{I-T}\\right)^{-1} \\vec{c}$$\n",
    "\n",
    "Therefore, $\\vec{x}^k$ converges to $\\left(\\mathbf{I-T}\\right)^{-1}\\vec{c}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "hide_input": true,
    "init_cell": true,
    "javascript_last_cell": true,
    "jupyter": {
     "source_hidden": true
    },
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "remove_cell"
    ]
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "function hideElements(elements, start) {\n",
       "for(var i = 0, length = elements.length; i < length;i++) {\n",
       "    if(i >= start) {\n",
       "        elements[i].style.display = \"none\";\n",
       "    }\n",
       "}\n",
       "}\n",
       "var prompt_elements = document.getElementsByClassName(\"prompt\");\n",
       "hideElements(prompt_elements, 0)\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "function hideElements(elements, start) {\n",
    "for(var i = 0, length = elements.length; i < length;i++) {\n",
    "    if(i >= start) {\n",
    "        elements[i].style.display = \"none\";\n",
    "    }\n",
    "}\n",
    "}\n",
    "var prompt_elements = document.getElementsByClassName(\"prompt\");\n",
    "hideElements(prompt_elements, 0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  },
  "rise": {
   "auto_select": null,
   "autolaunch": false,
   "controls": true,
   "enable_chalkboard": false,
   "footer": "",
   "progress": true,
   "scroll": true,
   "slideNumber": false,
   "theme": "simple"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
